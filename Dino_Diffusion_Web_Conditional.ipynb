{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky0RHfx1buVD"
      },
      "source": [
        "# Objective\n",
        "\n",
        "This notebook trains a bare-bones diffusion model (\"Dino Diffusion\") to unconditionally generate images.\n",
        "\n",
        "To do this, it trains a simple neural network that denoises patches of images. This network is then reused across space and time, to generate new samples starting from pure noise.\n",
        "\n",
        "The main sources of complexity (not in the base dino diffusion notebook) are:\n",
        "1. Sketch conditioning (consuming a sketch that shows what to draw)\n",
        "2. Cascaded upsampling (upsampling low-resolution generations into higher-resolution ones)\n",
        "\n",
        "I recommend understanding the base notebook first.\n",
        "\n",
        "Also TODO: this notebook is way messier and harder to read, sorry :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXzEyXx-36QU"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtJSiiuF382r"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = \"retina\"\n",
        "\n",
        "import random\n",
        "from collections import namedtuple\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms.functional as tv\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "th.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5maBpmc5OPM"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    device = \"cuda\" if th.cuda.is_available() else \"cpu\"\n",
        "    channels = 3\n",
        "    patch_hw = 64\n",
        "    upscale_factor = 2\n",
        "    dataset = \"pokemon\" #\"public_domain_plants_tiny\"\n",
        "\n",
        "def show(x):\n",
        "    if not isinstance(x, th.Tensor) or x.ndim == 4:\n",
        "        x = th.cat(tuple(x), -1)\n",
        "    display(tv.to_pil_image(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeAVO9AmriJj"
      },
      "source": [
        "# Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(name):\n",
        "    if Path(name).exists():\n",
        "        print(f\"dataset '{name}' already exists; skipping...\")\n",
        "        return\n",
        "    !git clone https://huggingface.co/datasets/huggan/{name} && (cd {name} && git lfs pull)\n",
        "    import pyarrow.parquet as pq\n",
        "    from io import BytesIO\n",
        "\n",
        "    i = 0\n",
        "    for table in Path(f\"{name}/data\").glob(\"*.parquet\"):\n",
        "        for row in tqdm(pq.read_table(table)[0]):\n",
        "            Image.open(BytesIO(row[\"bytes\"].as_py())).save(f\"{name}/{i:04d}.jpg\")\n",
        "            i += 1\n",
        "\n",
        "get_dataset(Config.dataset)"
      ],
      "metadata": {
        "id": "t5NaZzkeBmg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgQfFMNY32yd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from torchvision.transforms import InterpolationMode\n",
        "from functools import lru_cache\n",
        "from PIL import ImageFilter\n",
        "\n",
        "Patch = namedtuple(\"Patch\", (\"patch\", \"coords\", \"cond\"))\n",
        "\n",
        "def coords_for_patch(x, y, width, height):\n",
        "    cx = th.linspace(x, x + width, Config.patch_hw).view(1, -1).expand(Config.patch_hw, Config.patch_hw)\n",
        "    cy = th.linspace(y, y + height, Config.patch_hw).view(-1, 1).expand(Config.patch_hw, Config.patch_hw)\n",
        "    cz = th.ones_like(cx)\n",
        "    return th.stack((cx, cy, cz))\n",
        "\n",
        "def threshold_cond(img):\n",
        "    img = np.array(img.convert(\"RGB\"))\n",
        "    return Image.fromarray(((np.array(img) > random.randint(0, 192)).astype(np.uint8) * 255))\n",
        "\n",
        "def box_draw(img_th):\n",
        "    c, h, w = img_th.shape\n",
        "    for i in range(random.randint(0, 3)):\n",
        "        bh, bw = random.randrange(1, h), random.randrange(1, w)\n",
        "        y, x = 0, 0\n",
        "        if bh < h: y = random.randrange(0, h - bh)\n",
        "        if bw < w: x = random.randrange(0, w - bw)\n",
        "        img_th[:, y:y+bh,x:x+bw] = 255\n",
        "    return img_th\n",
        "\n",
        "def process_conditioning(img):\n",
        "    arr = np.array(img.convert(\"RGB\"))[..., :3]\n",
        "    linework = (arr < random.randint(20, 100)).all(-1)\n",
        "    img = Image.fromarray((~linework * 255).astype(np.uint8))\n",
        "    img = img.filter(ImageFilter.MedianFilter(3))\n",
        "    return img\n",
        "\n",
        "class Dataset:\n",
        "    def __init__(self, p):\n",
        "        self.ims = []\n",
        "        if not isinstance(p, (tuple, list)):\n",
        "            p = [p]\n",
        "        for pi in p:\n",
        "            self.ims.extend(Path(pi).rglob(\"*.png\"))\n",
        "            self.ims.extend(Path(pi).rglob(\"*.jpg\"))\n",
        "    def __len__(self):\n",
        "        return len(self.ims)\n",
        "    def __getitem__(self, i, aug_rotation=True):\n",
        "        img = Image.open(self.ims[i])\n",
        "        cond = None\n",
        "        if random.random() < 0.5:\n",
        "            cond = process_conditioning(img)\n",
        "        patch_scale = random.random()\n",
        "        crop_hw = Config.patch_hw + int(patch_scale * (min(img.size) - Config.patch_hw))\n",
        "        crop_x = random.randrange(0, img.width - crop_hw) if img.width > crop_hw else 0\n",
        "        crop_y = random.randrange(0, img.height - crop_hw) if img.height > crop_hw else 0\n",
        "        crop = img.crop((crop_x, crop_y, crop_x + crop_hw, crop_y + crop_hw))\n",
        "        if cond is not None:\n",
        "            cond = cond.crop((crop_x, crop_y, crop_x + crop_hw, crop_y + crop_hw))\n",
        "        coords = coords_for_patch(crop_x / img.width, crop_y / img.height, crop_hw / img.width, crop_hw / img.height)\n",
        "        coords = (coords * 255).round().byte()\n",
        "        if aug_rotation and crop.width > 3 * Config.patch_hw and crop.height > 3 * Config.patch_hw and random.random() < 0.5:\n",
        "            patch = crop.resize((Config.patch_hw * 3, Config.patch_hw * 3)).convert(\"RGB\")\n",
        "            if cond is not None:\n",
        "                cond = cond.resize(patch.size).convert(\"RGB\")\n",
        "            coords = tv.resize(coords, patch.size)\n",
        "            angle = random.random() * (20) - 10\n",
        "            uncropped_size = (int(Config.patch_hw * 1.5), int(Config.patch_hw * 1.5))\n",
        "            patch = tv.center_crop(tv.resize(tv.rotate(patch, angle, InterpolationMode.BILINEAR), uncropped_size, InterpolationMode.NEAREST), (Config.patch_hw, Config.patch_hw))\n",
        "            coords = tv.center_crop(tv.resize(tv.rotate(coords, angle, InterpolationMode.BILINEAR), uncropped_size), (Config.patch_hw, Config.patch_hw))\n",
        "            if cond is not None:\n",
        "                cond = tv.center_crop(tv.resize(tv.rotate(cond, angle, InterpolationMode.BILINEAR), uncropped_size), (Config.patch_hw, Config.patch_hw))\n",
        "        else:\n",
        "            patch = crop.resize((Config.patch_hw, Config.patch_hw))\n",
        "            if cond is not None:\n",
        "                cond = cond.resize((Config.patch_hw, Config.patch_hw))\n",
        "        if random.random() < 0.5:\n",
        "            patch, coords = tv.hflip(patch), tv.hflip(coords)\n",
        "            if cond is not None:\n",
        "                cond = tv.hflip(cond)\n",
        "        cond = 255 * th.ones(3, Config.patch_hw, Config.patch_hw, dtype=th.uint8) if cond is None else box_draw(tv.pil_to_tensor(threshold_cond(cond)))\n",
        "        return Patch(tv.pil_to_tensor(patch), coords, cond)\n",
        "\n",
        "d_train = Dataset(Config.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJ6QsO5YAY_S"
      },
      "outputs": [],
      "source": [
        "def demo_dataset(dataset, n=16):\n",
        "    print(f\"Dataset has {len(dataset)} images.\")\n",
        "    print(f\"Here are some sample patches from the dataset:\")\n",
        "    samples = random.choices(dataset, k=n)\n",
        "    show(s.patch for s in samples)\n",
        "    show(s.coords for s in samples)\n",
        "    show(s.cond for s in samples)\n",
        "\n",
        "demo_dataset(d_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1vprylFBpWk"
      },
      "source": [
        "# Model\n",
        "\n",
        "Next, we define the neural network.\n",
        "\n",
        "It will take in a noisy patch, the % of noise, the patch coordinates, and an optional low-resolution patch, and try to predict the corresponding (denoised) high-resolution patch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7u7MqTNaE-rV"
      },
      "outputs": [],
      "source": [
        "Prediction = namedtuple(\"Prediction\", (\"denoised\"))\n",
        "\n",
        "class Blocks(nn.Module):\n",
        "    def __init__(self, n_in, n_f, n_out, n_l, bias=False):\n",
        "        super().__init__()\n",
        "        self.cs = nn.ModuleList()\n",
        "        for i in range(n_l):\n",
        "            self.cs.append(nn.Sequential(\n",
        "                nn.Conv2d(n_in if i == 0 else n_f, n_f, 3, padding = 1), nn.ReLU(),\n",
        "                nn.Conv2d(n_f, n_f, 3, padding=1), nn.ReLU(),\n",
        "            ))\n",
        "        self.s = nn.Conv2d(n_in + n_f * n_l, n_out, 1, bias=bias)\n",
        "    def forward(self, x):\n",
        "        out = [x]\n",
        "        for b in self.cs:\n",
        "            x = b(x)\n",
        "            out.append(x)\n",
        "        return self.s(th.cat(out, 1))\n",
        "def EncBlock(n_in, n_out, n_b):\n",
        "    return nn.Sequential(nn.Conv2d(n_in, n_out, 4, stride=2, padding=1, bias=False), Blocks(n_out, n_out, n_out, n_b))\n",
        "\n",
        "def DecBlock(n_in, n_out, n_b):\n",
        "    return nn.Sequential(Blocks(n_in, n_out, n_out * 4, n_b, bias=False), nn.PixelShuffle(2))\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_in_out=Config.channels, n_f=[64, 64, 64, 64, 64], n_b=[1, 2, 2, 2, 2]):\n",
        "        super().__init__()\n",
        "        self.c_cat = Blocks(n_in_out * 4 + 1, n_f[0], n_f[0], n_b[0])\n",
        "        self.c_enc = nn.ModuleList([EncBlock(n_f[i-1], n_f[i], n_b[i]) for i in range(1, len(n_f))])\n",
        "        self.c_dec = nn.ModuleList([DecBlock(n_f[-i] * (1 if i == 1 else 2), n_f[-i-1], n_b[-i]) for i in range(1, len(n_f))])\n",
        "        self.c_out = nn.Sequential(Blocks(n_f[0] * 2, n_f[0], n_f[0], n_b[0]), nn.Conv2d(n_f[0], n_in_out, 3, padding=1))\n",
        "        nn.init.constant_(self.c_out[-1].bias, 0.5)\n",
        "    def forward(self, x, noise_level, x_lowres, x_coords, x_cond):\n",
        "        x = self.c_cat(th.cat([x, x_lowres, x_coords, x_cond, noise_level + 0 * x[:, :1]], 1))\n",
        "        skips = []\n",
        "        for c_enc in self.c_enc:\n",
        "            skips.append(x)\n",
        "            x = c_enc(x)\n",
        "        for c_dec in self.c_dec:\n",
        "            x = th.cat([c_dec(x), skips.pop()], 1)\n",
        "        return Prediction(self.c_out(x))\n",
        "\n",
        "model = UNet().to(Config.device)\n",
        "\n",
        "# Our final model will use a smoothed average of recent weights\n",
        "# (this is a ~free way to get a higher-quality final model)\n",
        "def weight_average(w_prev, w_new, _):\n",
        "    return 0.9 * w_prev + 0.1 * w_new\n",
        "avg_model = th.optim.swa_utils.AveragedModel(model, avg_fn=weight_average)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1qPL17fOOy4"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "@th.no_grad()\n",
        "def demo_model(model, n=16):\n",
        "    model.eval()\n",
        "    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Model has {n_parameters / 1e6:.1f} million trainable parameters ({4 * n_parameters / 1e6:.1f} MB).\")\n",
        "    x, x_lowres, x_coords, x_cond = (th.rand(n, Config.channels, Config.patch_hw, Config.patch_hw, device=Config.device) for _ in \"rawr\")\n",
        "    noise_level = th.rand(n, 1, 1, 1, device=Config.device)\n",
        "    tick = time.time()\n",
        "    for _ in range(10):\n",
        "        y = model(x, noise_level, x_lowres, x_coords, x_cond)\n",
        "    tock = time.time()\n",
        "    print(f\"Here are some model outputs on random noise in {100 * (tock - tick):.1f}ms:\")\n",
        "    show(y.denoised.clamp(0, 1))\n",
        "    model.train()\n",
        "\n",
        "demo_model(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELQ4LAkzRwDL"
      },
      "source": [
        "# Diffusion Training Logic\n",
        "\n",
        "To train our diffusion model, we need some code that adds noise to patches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9ktK3jnSidk"
      },
      "outputs": [],
      "source": [
        "NoisyPatch = namedtuple(\"NoisyPatch\", (\"patch\", \"noisy_patch\", \"noise\", \"noise_level\", \"lowres\", \"coords\", \"cond\", \"has_lowres\"))\n",
        "\n",
        "def alpha_blend(a, b, alpha):\n",
        "    return alpha * a + (1 - alpha) * b\n",
        "\n",
        "def mix_blur(x, alpha):\n",
        "    return alpha_blend(F.avg_pool2d(x, 3, padding=1, count_include_pad=False, stride=1), x, alpha)\n",
        "\n",
        "def augmented_lowres(x, ds=2):\n",
        "    x_avg = F.avg_pool2d(x, ds, stride=ds)\n",
        "    blur_alpha = F.interpolate(th.rand(x_avg.shape[0], 1, x_avg.shape[-2]//4, x_avg.shape[-1]//4, device=x_avg.device), scale_factor=4, mode=\"bilinear\")\n",
        "    noise_alpha = F.interpolate(th.rand(x_avg.shape[0], 1, x_avg.shape[-2]//4, x_avg.shape[-1]//4, device=x_avg.device), scale_factor=4, mode=\"bilinear\")\n",
        "    x_down = mix_blur(x_avg, blur_alpha)\n",
        "    x_down = x_down + 0.02 * noise_alpha * th.rand_like(x_down[:, :1, :1, :1]) * th.randn_like(x_down)\n",
        "    return F.interpolate(x_down, scale_factor=ds)\n",
        "\n",
        "@th.no_grad()\n",
        "def add_noise_to_patches(patches, lowres_dropout=0.75):\n",
        "    patch, coords, cond = (x.to(Config.device) / 255.0 for x in patches)\n",
        "    noise_level = th.rand_like(patch[:, :1, :1, :1])\n",
        "    dropout_mask = th.rand_like(noise_level) < lowres_dropout\n",
        "    lowres = dropout_mask * -1 + ~dropout_mask * augmented_lowres(patch)\n",
        "    noise = th.rand_like(patch)\n",
        "    noisy_patch = alpha_blend(noise, patch, noise_level)\n",
        "    return NoisyPatch(patch, noisy_patch, noise, noise_level, lowres, coords, cond, ~dropout_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZerWknmKaFag"
      },
      "outputs": [],
      "source": [
        "def demo_data_generation(dataset, n_demo=16):\n",
        "    patches = next(iter(th.utils.data.DataLoader(dataset, batch_size=n_demo, shuffle=True)))\n",
        "    print(\"Here's what the targets look like during training\")\n",
        "    print(\"Here's what the inputs look like during training\")\n",
        "    patches = add_noise_to_patches(patches)\n",
        "    show(patches.patch)\n",
        "    show(patches.noisy_patch)\n",
        "    show(patches.noise_level.expand(n_demo, 3, 16, Config.patch_hw))\n",
        "    show(patches.lowres.clamp(0, 1))\n",
        "    show(patches.coords)\n",
        "    show(patches.cond)\n",
        "\n",
        "demo_data_generation(d_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Utzy-yt8kWOr"
      },
      "source": [
        "# Diffusion Sampling Logic\n",
        "\n",
        "To sample from our diffusion model, we need some code that iteratively removes noise from a given patch using the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HAjLpaqkPs9"
      },
      "outputs": [],
      "source": [
        "@th.no_grad()\n",
        "def generate_images_at_resolution(model, lowres_patches, coords, n_steps, generator=None):\n",
        "    pure_noise = th.rand(*lowres_patches.shape, device=Config.device, generator=generator)\n",
        "    x = pure_noise\n",
        "    noise_levels = th.linspace(1, 0, n_steps + 1, device=Config.device)\n",
        "    cond = th.ones_like(lowres_patches)\n",
        "    for nl_in, nl_out in zip(noise_levels, noise_levels[1:]):\n",
        "        denoised_patches = model(x, nl_in.expand(x[:, :1, :1, :1].shape), lowres_patches, coords, cond).denoised\n",
        "        x = alpha_blend(x, denoised_patches, nl_out / nl_in)\n",
        "    return x.clamp(0, 1)\n",
        "\n",
        "@th.no_grad()\n",
        "def generate_images(model, n_images=4, n_steps_per_resolution=[50], generator=None):\n",
        "    model.eval()\n",
        "    lowres_patches = -th.ones(n_images, Config.channels, Config.patch_hw, Config.patch_hw, device=Config.device)\n",
        "    coords = coords_for_patch(0, 0, 1, 1).unsqueeze(0).to(Config.device).expand(lowres_patches.shape)\n",
        "    for n_steps in n_steps_per_resolution:\n",
        "        patches = generate_images_at_resolution(model, lowres_patches, coords, n_steps, generator=generator)\n",
        "        lowres_patches = F.interpolate(patches, scale_factor=Config.upscale_factor)\n",
        "        coords = F.interpolate(coords, scale_factor=Config.upscale_factor, mode=\"bilinear\", align_corners=False)\n",
        "    model.train()\n",
        "    return patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cGKrZLlcwXG"
      },
      "outputs": [],
      "source": [
        "def demo_image_generation(model, n_demo=4, seed=9):\n",
        "    print(\"Here are some samples from the model\")\n",
        "    def rng(): return th.Generator(device=Config.device).manual_seed(seed)\n",
        "    show(generate_images(model, n_images=16, n_steps_per_resolution=[50], generator=rng()).clamp(0, 1))\n",
        "    show(generate_images(model, n_images=8, n_steps_per_resolution=[50, 5], generator=rng()).clamp(0, 1))\n",
        "    show(generate_images(model, n_images=4, n_steps_per_resolution=[50, 20, 10], generator=rng()).clamp(0, 1))\n",
        "    show(generate_images(model, n_images=2, n_steps_per_resolution=[50, 10, 5, 1], generator=rng()).clamp(0, 1))\n",
        "demo_image_generation(avg_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz_1RFozi-bs"
      },
      "source": [
        "# Training\n",
        "\n",
        "Finally, we write a training loop that loads patches from the dataset, adds noise to them, and trains our neural network to remove the noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpJPeGuYU_VN"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from IPython.display import clear_output\n",
        "import time\n",
        "import datetime\n",
        "def _mean(x):\n",
        "    return sum(x) / len(x)\n",
        "\n",
        "class Repeat(th.utils.data.Dataset):\n",
        "    def __init__(self, dataset, n=1<<20):\n",
        "        self.dataset = dataset\n",
        "        self.n = n\n",
        "    def __len__(self):\n",
        "        return self.n\n",
        "    def __getitem__(self, i):\n",
        "        return self.dataset[i % len(self.dataset)]\n",
        "\n",
        "class TinyProfiler:\n",
        "    def __init__(self):\n",
        "        self.t = {}\n",
        "        self.r = defaultdict(list)\n",
        "    def tick(self, k):\n",
        "        self.t[k] = time.time();\n",
        "    def tock(self, k):\n",
        "        if k not in self.t:\n",
        "            raise ValueError(k)\n",
        "        elapsed = time.time() - self.t[k]\n",
        "        self.r[k].append(elapsed)\n",
        "    def __repr__(self):\n",
        "        return \"Timing:\\n\"+ \"\\n\".join(f\"{'  ' * k.count('_')}{k.split('_')[-1].ljust(16)}: \\033[34m{1000 * _mean(self.r[k][-10:]):0.1f}\\033[0m\\033[37mms\\033[0m\" for k in sorted(self.r))\n",
        "\n",
        "def init_log_path():\n",
        "    i = 0\n",
        "    log_path = datetime.datetime.now().strftime(\"%Y_%m_%d\") + \"_dino_diffusion\"\n",
        "    while Path(log_path + f\"_{i}\").exists() and len(list(Path(log_path + f\"_{i}\").glob(\"*.jpg\"))) > 1:\n",
        "        i += 1\n",
        "    log_path += f\"_{i}\"\n",
        "    print(log_path)\n",
        "    !mkdir -p \"$log_path\"\n",
        "    return log_path\n",
        "\n",
        "class DinoTrainer:\n",
        "    def __init__(self, model, dataset, ema_model=None, batch_size=16, n_demo=16, lr=3e-4):\n",
        "        self.stats = defaultdict(list)\n",
        "        self.stat_steps = defaultdict(list)\n",
        "        self.model = model\n",
        "        self.ema_model = ema_model\n",
        "        self.opt = th.optim.AdamW(self.model.parameters(), lr, amsgrad=True)\n",
        "        self.dl = th.utils.data.DataLoader(Repeat(dataset), batch_size=batch_size, shuffle=True, drop_last=True, num_workers=8)\n",
        "        self.batch_size = batch_size\n",
        "        self.n_demo = n_demo\n",
        "        self.log_path = init_log_path()\n",
        "        self.prof = TinyProfiler()\n",
        "        self.step = 0\n",
        "        self.generator = th.Generator(device=Config.device)\n",
        "        # adversarial loss to try and make upsampling model generate images faster\n",
        "        self.adv_model = nn.Sequential(\n",
        "            nn.Conv2d(3, 8, 4, stride=2), nn.ReLU(),\n",
        "            nn.Conv2d(8, 16, 4, stride=2, groups=2), nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, 3, groups=4), nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 3, groups=4), nn.ReLU(), nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(64, 1, 1)\n",
        "        ).to(Config.device)\n",
        "        self.adv_opt = th.optim.AdamW(self.adv_model.parameters(), lr)\n",
        "\n",
        "    def loss(self, pred, gt, noise_level, has_lowres):\n",
        "        a = self.adv_model(pred)\n",
        "        with th.no_grad():\n",
        "            b = self.adv_model(gt)\n",
        "        # adversarial loss is only applied when upsampling\n",
        "        return F.mse_loss(pred, gt) + 0.25 * th.pow(has_lowres * (a - b), 2).mean()\n",
        "\n",
        "    def train(self, n_steps=1000_000, render_interval_s=30, demo_interval_s=60, save_interval_s=300, runtime_s=15*60, weight_avg_interval_steps=100):\n",
        "        start = time.time()\n",
        "        last_disp_time = 0\n",
        "        last_demo_time = -1000\n",
        "        last_save_time = -1000\n",
        "        last_render_time = -1000\n",
        "\n",
        "        dl_gen = iter(self.dl)\n",
        "        loss_acc = []\n",
        "        for rel_step in range(1, 1 + n_steps):\n",
        "            self.step += 1\n",
        "            seconds_elapsed = (time.time() - start)\n",
        "            render_step = seconds_elapsed > last_render_time + render_interval_s\n",
        "\n",
        "            self.prof.tick(\"step\")\n",
        "\n",
        "            self.prof.tick(\"dl\")\n",
        "            self.prof.tick(\"dl_next\")\n",
        "            try:\n",
        "                xb = next(dl_gen)\n",
        "            except StopIteration:\n",
        "                dl_gen = iter(self.dl)\n",
        "                xb = next(dl_gen)\n",
        "            self.prof.tock(\"dl_next\")\n",
        "            self.prof.tick(\"dl_copy\")\n",
        "            xb = add_noise_to_patches(xb)\n",
        "            self.prof.tock(\"dl_copy\")\n",
        "            self.prof.tock(\"dl\")\n",
        "\n",
        "            self.prof.tick(\"model\")\n",
        "\n",
        "            self.prof.tick(\"model_grad\")\n",
        "            self.model.train()\n",
        "            xpb = self.model(xb.noisy_patch, xb.noise_level, xb.lowres, xb.coords, xb.cond).denoised\n",
        "            if render_step:\n",
        "                grad_catcher = th.zeros_like(xpb).requires_grad_(True)\n",
        "                xpb = xpb + grad_catcher\n",
        "            loss = self.loss(xpb, xb.patch, xb.noise_level, xb.has_lowres)\n",
        "            loss_acc.append(loss.item())\n",
        "            self.prof.tock(\"model_grad\")\n",
        "            self.prof.tick(\"model_bwd\")\n",
        "            self.opt.zero_grad(); loss.backward(); self.opt.step()\n",
        "            self.prof.tock(\"model_bwd\")\n",
        "            self.prof.tock(\"model\")\n",
        "\n",
        "            # update adversarial trainer\n",
        "            adv_loss = (\n",
        "                F.mse_loss(self.adv_model(xpb.detach()), xb.noise_level) +\n",
        "                F.mse_loss(self.adv_model(xb.patch), 0 * xb.noise_level)\n",
        "            )\n",
        "            self.adv_opt.zero_grad(); adv_loss.backward(); self.adv_opt.step()\n",
        "\n",
        "            self.prof.tock(\"step\")\n",
        "\n",
        "            if self.ema_model is not None and rel_step % weight_avg_interval_steps == 0:\n",
        "                self.ema_model.update_parameters(self.model)\n",
        "\n",
        "            # steps per minute\n",
        "            minutes_elapsed = seconds_elapsed / 60\n",
        "            est_time = n_steps / rel_step * minutes_elapsed\n",
        "            est_time_str = f\"\\033[35m{est_time:.3f}\\033[0m minutes\" if est_time < 60 else f\"\\033[35m{est_time / 60:.1f}\\033[0m hours\"\n",
        "            if render_step:\n",
        "                # losses as usual\n",
        "                self.stats[\"model_loss\"].append(sum(loss_acc) / len(loss_acc))\n",
        "                loss_acc = []\n",
        "                self.stat_steps[\"model_loss\"].append(self.step)\n",
        "                last_render_time = seconds_elapsed\n",
        "                self.generator.manual_seed(1337)\n",
        "                demo_img = tv.to_pil_image(th.cat(tuple(generate_images(self.model, n_images=16, n_steps_per_resolution=[32], generator=self.generator)), -1).clamp(0, 1))\n",
        "                if self.ema_model is not None:\n",
        "                    self.generator.manual_seed(1337)\n",
        "                    demo_img_ema = tv.to_pil_image(th.cat(tuple(generate_images(self.ema_model, n_images=16, n_steps_per_resolution=[32], generator=self.generator)), -1).clamp(0, 1))\n",
        "                self.generator.manual_seed(1337)\n",
        "                demo_img_hr = tv.to_pil_image(th.cat(tuple(generate_images(self.ema_model, n_images=4, n_steps_per_resolution=[32, 16, 8], generator=self.generator)), -1).clamp(0, 1))\n",
        "                clear_output(wait=True)\n",
        "                print(\"demo on fixed noise\")\n",
        "                display(demo_img)\n",
        "                if self.ema_model is not None:\n",
        "                    display(demo_img_ema)\n",
        "                display(demo_img_hr)\n",
        "                with th.no_grad():\n",
        "                    print(\"model training\")\n",
        "                    mask = th.randperm(len(xb.patch))[:self.n_demo].sort(0).values\n",
        "                    print(\"input (input_nl, input_lr, input_coords, input)\")\n",
        "                    display(tv.to_pil_image(th.cat(tuple(xb.noise_level.detach().expand(xb.patch.shape)[mask, :3, :16]), -1)))\n",
        "                    display(tv.to_pil_image(th.cat(tuple(xb.lowres.detach()[mask, :3]), -1).clamp(0, 1)))\n",
        "                    display(tv.to_pil_image(th.cat(tuple(xb.coords.detach()[mask, :3]), -1).clamp(0, 1)))\n",
        "                    display(tv.to_pil_image(th.cat(tuple(xb.cond.detach()[mask, :3]), -1).clamp(0, 1)))\n",
        "                    display(tv.to_pil_image(th.cat(tuple(xb.noisy_patch.detach()[mask, :3]), -1).clamp(0, 1)))\n",
        "                    print(\"pred\")\n",
        "                    display(tv.to_pil_image(th.cat(tuple(xpb.detach()[mask]), -1).clamp(0, 1)))\n",
        "                    print(\"target\")\n",
        "                    display(tv.to_pil_image(th.cat(tuple(xb.patch.detach()[mask, :3]), -1)))\n",
        "                    grad_im = grad_catcher.grad.detach()\n",
        "                    print(f\"grad (min {grad_im.min().item():.6f} mean {grad_im.mean().item():.6f} max {grad_im.max().item():.6f})\")\n",
        "                    # show flipped gradient\n",
        "                    grad_im = 0.5 - grad_im / (th.abs(grad_im).max() + 1e-6)\n",
        "                    display(tv.to_pil_image(th.cat(tuple(grad_im[mask]), -1).clamp(0, 1)))\n",
        "                if seconds_elapsed > last_demo_time + demo_interval_s:\n",
        "                    demo_img.save(f\"{self.log_path}/{self.step:06d}.jpg\", quality=95)\n",
        "                    if self.ema_model is not None:\n",
        "                        demo_img_ema.save(f\"{self.log_path}/ema_{self.step:06d}.jpg\", quality=95)\n",
        "                    demo_img_hr.save(f\"{self.log_path}/hr_{self.step:06d}.jpg\", quality=95)\n",
        "                    th.save((self.stats, self.stat_steps), f\"{self.log_path}/stats.pth\")\n",
        "                    last_demo_time = seconds_elapsed\n",
        "                if seconds_elapsed > last_save_time + save_interval_s:\n",
        "                    print(\"saving checkpoints...\")\n",
        "                    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "                    th.save(self.model.state_dict(), f\"{self.log_path}/model_checkpoint_last.pth\")\n",
        "                    th.save(self.ema_model.state_dict(), f\"{self.log_path}/ema_model_checkpoint_last.pth\")\n",
        "                    export_model(self.ema_model, f\"{self.log_path}/ema_model_checkpoint_last.onnx\")\n",
        "                    print(\"saved\")\n",
        "                    last_save_time = seconds_elapsed\n",
        "                plt.title(\"Stats\")\n",
        "                for k, vs in self.stats.items():\n",
        "                    plt.plot(self.stat_steps[k], vs, label=f\"{k} ({vs[-1]:.5f})\", linewidth=2, alpha=0.75)\n",
        "                plt.ylim(0, 2 * max(self.stats[\"model_loss\"][-3:]))\n",
        "                plt.gcf().set_size_inches(12, 4)\n",
        "                plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
        "                plt.gcf().savefig(f\"{self.log_path}/stats.jpg\", bbox_inches='tight')\n",
        "                plt.show()\n",
        "                print(self.prof)\n",
        "                print()\n",
        "            if seconds_elapsed > last_disp_time + 1:\n",
        "                last_disp_time = seconds_elapsed\n",
        "                def make_pbar(prop, w=20):\n",
        "                    p = -int(-w * prop)\n",
        "                    return f\"\\033[42m\" + \" \" * p + \"\\033[40m\" + \" \" * (w - p) + \"\\033[0m\"\n",
        "                render_interval_s_pos = seconds_elapsed - last_render_time\n",
        "                print(\n",
        "                    \"\\r\"\n",
        "                    f\"{time.time() - start:.0f}s / {runtime_s:.0f}s. Step {self.step: 5d}. Demo in {render_interval_s - render_interval_s_pos:.1f}s \"\n",
        "                    f\"{make_pbar(render_interval_s_pos / render_interval_s)}; ðŸ¦” \\033[34m{rel_step / minutes_elapsed:.3f}\\033[0m Steps / Min. BS {self.batch_size}. \"\n",
        "                    f\"\\033[35m{self.batch_size * rel_step / minutes_elapsed:.3f}\\033[0m Images / Min. \"\n",
        "                    f\"Loss {loss.item():0.4f} \"\n",
        "                    f\"{th.cuda.memory_reserved() / 1e9:.2f}GB\"\n",
        "                , end=\"\")\n",
        "\n",
        "            if time.time() - start > runtime_s:\n",
        "                print(\"\\n âœ… Training completed\")\n",
        "                self.model.eval()\n",
        "                th.save(self.model.state_dict(), f\"{self.log_path}/model_checkpoint_last.pth\")\n",
        "                break\n",
        "\n",
        "trainer = DinoTrainer(model, d_train, ema_model=avg_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdJ15sftU_VN"
      },
      "outputs": [],
      "source": [
        "trainer.train(runtime_s=960) #1*60*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADS_a-MG4U8c"
      },
      "outputs": [],
      "source": [
        "show(generate_images(avg_model, n_images=16, n_steps_per_resolution=[50]).clamp(0, 1))\n",
        "show(generate_images(avg_model, n_images=8, n_steps_per_resolution=[32, 8]).clamp(0, 1))\n",
        "show(generate_images(avg_model, n_images=4, n_steps_per_resolution=[32, 10, 1]).clamp(0, 1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx"
      ],
      "metadata": {
        "id": "ixSpUOI6CTsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fV6eEBULU_VM"
      },
      "outputs": [],
      "source": [
        "def export_model(model, name=\"model_test.onnx\"):\n",
        "    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Model has {n_parameters / 1e6:.1f} million trainable parameters.\")\n",
        "    x, x_lowres, x_coords, x_cond = (th.rand(1, Config.channels, Config.patch_hw, Config.patch_hw, device=Config.device) for _ in \"rawr\")\n",
        "    noise_level = th.rand(1, 1, 1, 1, device=Config.device)\n",
        "    th.onnx.export(model, (x, noise_level, x_lowres, x_coords, x_cond), name,\n",
        "                   input_names=[\"x\", \"noise_level\", \"x_lowres\", \"x_coords\", \"x_cond\"],\n",
        "                   output_names=[\"denoised\"]\n",
        "                   , opset_version=9)\n",
        "    print(\"Exported model to\", name)\n",
        "\n",
        "export_model(model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}